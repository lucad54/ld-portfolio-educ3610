---
title: Intellectual Productions
layout: about
permalink: /production3.html
# include CollectionBuilder info at bottom
credits: false
# Edit the markdown on in this file to describe your collection
# Look in _includes/feature for options to easily add features to the page
---

{% include feature/jumbotron.html objectid="background01" %}

{% include feature/nav-menu.html sections="Production 3 - GenAI Essay;References" %}

## Production 3 - GenAI Essay

#### Intro 
For this production, I selected to use OpenAI's ChatGPT as it is one of the AI models "at the forefront of promoting AI as a solution to some of the biggest challenges in education" (Birhan, 2025, p. 54). As my teachable is history, I started my probe into AI with the prompt “simulate (role play) a person or historical actor from the past and engage in dialogue about a specific historical event/situation or controversy,” following the example direction from Production 3. More specifically, I prompted ChatGPT to role-play Julius Caesar in a dialogue about his assassination on the Ides of March in 44 BCE. My purpose was not to see whether the AI could “create” a text on/by a historical figure, but to see how accurate and/or harmful its output was in relation to history education. My hope for this production is to reveal both the potential and dangers of AI in history education, exposing its biases and inaccuracies as a historical source and its part in reproducing colonial epistemologies.

{% include feature/image.html objectid="chat01" %}

#### My Contact
I began my contact with OpenAI with a vague prompt, as seen above, to see the basic abilities of ChatGPT without strict guidance. After the first prompt, I quickly saw its historical inaccuracies, insinuating that Caesar knew what was going to happen to him on the “Ides of March”. The response was light-hearted, it was in English and was almost childishly asking if we “would have me heed these omens, or stride forth as Rome’s destined ruler?” as if I had a say in the past, especially in a Roman senate. My next step was to push the AI to fix these loose generalizations and cliches, prompting it to “speak the language of the time and not act as if Caesar knew what was going to happen, no childish play either”. The AI first apologized and tried to comfort me; however, even after assuring that it “understood” my requests (as seen below), it quickly became clear that it continued to ignore my prompts.

{% include feature/image.html objectid="chat02" %}

I then more specifically prompted the AI to “speak Latin, and ignore the 'audience’, roleplaying with the senate”. This finally began to fix the algorithmic bias of ChatGPT of speaking English. Still, I now faced the problem of not understanding Latin, so I went back and fixed my prompt to “speak Latin, and ignore the 'audience’, roleplaying with the senate (with English translations after)”. This worked, and the AI seemed to be using historical quotes from the senate before Caesar's assassination, but I suspected that these quotes were too "perfect" for the scenario prompted. So, to double check, I simply asked the AI where it was getting information from. ChatGPT quickly defends its “work” by saying it's not ancient text but a “historical simulation”. It follows itself, stating several “primary sources” as seen below.

{% include feature/image.html objectid="chat04" %}

#### Learning Potential
Through this production, I learned that with proper and strict guidelines, ChatGPT could produce somewhat “historical” literature, which is prone to "hallucination”. For example, when I asked where its bold Latin speeches came from, it admitted they were not translations of ancient texts but “historical simulations” with primary sources from that period. It pulled *ideas* from sources like Suetonius and Cicero, a historical writer and a contemporary, respectively. This honesty is useful, but it also exposes the risk that students and/or teachers might mistake these reconstructions for primary sources if they are not critically examining the AI. I found this connected to Thumlert et al.’s discussion of “algorithmic world-making", explaining how algorithms do not simply provide neutral information but co-produce with users a decontextualized version of reality (Thumlert et al., 2023, p. 23). In my experience with AI, it was not using Caesar's or other contemporaries’ words but generating a version of Caesar that was based on its training data and assumptions. This experiment significantly began to reveal the potential and limitations GenAI holds in a history classroom. 

{% include feature/image.html objectid="chat03" %}

#### Algorithmic Bias and Assumptions
As stated before, ChatGPT’s outputs quickly revealed biases shaped by its programming and dataset. When I asked to talk as an Ancient Roman leader, it did not speak Latin, but instead its first instinct was to create an almost Shakespearean English response. I learned that this is due to its default assumptions the AI's literary data has. I guess that the AI got lazy through its initial research around the 1500s, relying on Renaissance literary representations of Caesar (specifically “Julius Caesar” by Shakespeare) to re-create a historical figure. Only after repeated and specific corrections did it attempt Latin, which must be less represented in its dataset. Connecting again to Thumlert et al.'s argument, algorithms reflect “the goals, biases, prejudices, and values of their programmers” and training data (Thumlert et al., 2023, p. 20).

#### Risks for Education
Many students and teachers have turned to AI for “help” for subjects throughout education.Suppose they begin relying on ChatGPT for historical reconstructions instead of working with primary sources, oral and written histories, and lessons. In that case, learners will become “used to dependence on others (human or non-human) to make decisions for them” (Thumlert et al., 2023, p. 26). The AI-generated Caesar tries to mimic primary sources to seem “authentic” and could easily be mistaken as true by a younger student or a non-history teacher. If a student believes the words of AI and submits a paper with evidence based on an AI-historical figure, they would have fallen into the trap of GenAI and perpetuated an accidental fabrication of history. My interaction with ChatGPT displays this risk, as it pretends to use Caesar’s *Commentarii* or Cicero’s speeches as it creates its “own” passages to push its algorithmic rhetoric onto users.

Therefore, production 3 taught me that prompt literacy would be a necessary skill in order to even think of using AI models like ChatGPT and avoid "mis/disinformation, harmful ideologies and anti-science sentiments that have become part and parcel of the current digital knowledge ecosystem" (Birhan, 2025, p. 55). By tightening my prompts, I began to steer the AI away from its *default assumptions* of clichés and dramatizations, toward a somewhat decent historical reconstruction (Thumlert, 2025, Lecture 3). It highlights how AI requires existing expertise to use, as without my historical (and even technological) background, I might not have fought with AI so much about its inaccuracies and might have accepted its words as fact. The screenshots throughout my text display this progression, from simple, silly, English-only responses filled with dramatic foreshadowing to a slightly more scholarly-based Latin speech with translation. Thus, as "vendors market generative AI as a powerful technology with the capacity to solve a multitude of societal, structural, political and economic challenges", this experiment shows how AI algorithms, using simplicity and probability, dangerously push their own ideas onto their users (Birhan, 2025, p. 53). Altogether, moving foward we as teachers, must learn the dangers of AI, teaching prompt-literacy and with verification exercises that have students cite and fact-check any AI-generated information to build critical awareness and accountability.

This activity showed me ChatGPT’s strengths and weaknesses. With strict prompting, it can generate creative reconstructions, language practice, or entertaining engagement with the past. On the other hand, without constant correction, its biases and reliance on cultural tropes, its algorithmic assumptions, and its tendency to hallucinate the truth are all risks for education and risk the normalization of disinformation (Thumlert, 2025, Lecture 3). Historical educators, if they must, should accept AI as a tool for critical inquiry/practice rather than as a source of historical truth. We should follow Thumlert et al.'s ideas and must understand not only what the AI produces but also why and how it produces it, and Selwyn's quote that the "Use of AI systems and tools must be safe and effective for students" (Thumlert et al., 2023, p. 24–26 & Selwyn, 2024, p. 12). 

#### Conclusion

To conclude, I believe that an activity like this, where one compares AI-generated content with real historical evidence, is an effective way to raise awareness and educate students and teachers alike about the learning potential and risks of AI. Used critically, it can spark curiosity and reflection. Used uncritically, it risks simplifying history into clichés and de-skilling students and even teachers. So, in a world filled with mixed discursive and rhetorical positions on GenAI, my experiment with ChatGPT reveals the falsehood behind bringing historical figures *back to life*, especially for education. It exposes AI's algorithmic biases and default assumptions that distort history; worse, it removes "the actual actions...where learning happens," perpetuating colonial schooling systems by simplifying knowledge on Western sources and tropes, while silencing other voices and epistemologies that can address the problems of AI (Thumlert et al., 2023, p.30). 

## References

Birhan, A. (2025) The incomputable classroom: The limits and dangers of AI in education. In, *AI and the future of education.* Unesco. (Page 52). 

Selwyn, N. (2024). On the Limits of Artificial Intelligence (AI) in Education. *NTPK: Special Issue on Artificial Intelligence in Education*, 10, 3–14.

Thumlert, K., McBride, M., Tomin, B., Nolan, J., Lotherington, H., & Boreland, T. (2023). Algorithmic literacies: Identifying educational models and heuristics for engaging the challenge of algorithmic culture. *Digital Culture and Education.*

Thumlert, K. (2025, September 30). *Lecture 3 - Critical Algorithmic Literacies: Algorithmic Culture and GenAI. EDU 3610: New Media Literacies & Culture*. York University.

OpenAI. (2025). *ChatGPT* (Sept 22 version) [Large language model]. [https://chat.openai.com/](https://chat.openai.com/)

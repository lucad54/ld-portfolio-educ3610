---
title: Intellectual Productions
layout: about
permalink: /production3.html
# include CollectionBuilder info at bottom
credits: false
# Edit the markdown on in this file to describe your collection
# Look in _includes/feature for options to easily add features to the page
---

{% include feature/jumbotron.html objectid="background01" %}

{% include feature/nav-menu.html sections="Production 3 - GenAI Essay;References" %}

## Production 3 - GenAI Essay

#### Intro 
For this production, I selected to use OpenAI's ChatGPT as it is on of the AI models "at the forefront of promoting AI as a solution to some of the biggest challenges in education" (Birhan, 2025, p. 54). As my teachable history, I started my probe into AI with the prompt “simulate (role play) a person or historical actor from the past and engage in dialogue about a specific historical event/situation or controversy,” following the example direction from Production 3. More specifically, I prompted ChatGPT to role-play Julius Caesar in a dialogue about his assassination on the Ides of March in 44 BCE. My purpose was not to see whether the AI could “create” a text on/by a historical figure, but to see how accurate and/or harmful its output was in relation to history education. My hope for this production is to reveal both the potential and dangers of AI in history education, exposing its biases and inaccuracies as a historical source and its part in reproducing colonial epistemologies.

{% include feature/image.html objectid="chat01" %}

#### My Contact
After the first prompt, I quickly saw its historical inaccuracies, insinuating that Caesar knew what was going to happen to him on the “Ides of March”. The response was light-hearted, it was in English and was almost childishly asking if we “would have me heed these omens, or stride forth as Rome’s destined ruler?” as if we had a say in the past history. My next step was to push the AI to fix these loose generalizations and cliches, prompting it to “speak the language of the time and not act as if Caesar knew what was going to happen, no childish play either”. The AI basically first apologized and tried to comfort me however it quickly became clear that it continued to ignore my prompts, even after assuring that it “understood” my requests as seen below. 

{% include feature/image.html objectid="chat02" %}

I then more specifically prompted, “speak Latin, and ignore the 'audience’, roleplaying with the senate”. This finally fixed the algorithmic bias of ChatGPT of speaking English; however, I now faced the problem of not understanding Latin, so I went back and fixed my prompt to “speak Latin, and ignore the 'audience’, roleplaying with the senate (with English translations after)”. This worked, and the AI seemed to be using historical quotes from the senate before Caesar's assassination, but I suspected that these quotes were too "perfect" for the scenario prompted. So, I simply asked the AI where it was getting information from. ChatGPT quickly defends its “work” by saying it's not ancient text but a “historical simulation”. It follows itself, stating a number of “primary sources” as seen below.

{% include feature/image.html objectid="chat04" %}

#### Learning Potential
Through this production, I learned that with proper and strict guidelines, ChatGPT could produce somewhat “historical” texts, but it is prone to "hallucination”. For example, when I asked where its bold Latin speeches came from, it admitted they were not translations of ancient texts but “historical simulations” with primary sources from that period. It pulled *ideas* from sources like Suetonius and Cicero, a historical writer and a contemporary, respectively. This honesty is useful, but it also exposes the risk that students and/or teachers might mistake these reconstructions for primary sources if they are not critically examining the AI. I found this connected to Thumlert et al.’s discussion of “algorithmic world-making", explaining how algorithms do not simply provide neutral information but co-produce with users a decontextualized version of reality (Thumlert et al., 2023, p. 23). In my experience with AI, it was not using Caesar's or other contemporaries’ words but generating a version of Caesar that was based on its training data and assumptions. This exprienment began to reveal the potentials AI holds in a history classroom. 

{% include feature/image.html objectid="chat03" %}

#### Algorithmic Bias and Assumptions
As stated before, ChatGPT’s outputs quickly revealed biases shaped by its programming and dataset. When I asked to talk as a Roman leader, it did not speak Latin but instead its first instinct was to create an almost Shakespearean English response. This is connected to its default assumptions the AI's literary data has, getting lazy through its intial research around the 1500s, relying on Renaissance literary representations of Caesar (especially “Julius Caesar” by Shakespear) to create a historical figure. Only after repeated and specific corrections did it attempt Latin, which must be less represented in its dataset. Connecting again to Thumlert et al.'s argument, algorithms reflect “the goals, biases, prejudices, and values of their programmers” and training data (Thumlert et al., 2023, p. 20).

#### Risks for Education
Many students and teachers have turned to AI for “help” on a number of subjects and in education. If they begin relying on ChatGPT for historical reconstructions instead of working with primary sources, oral and written histories, and classrooms, learners become “used to dependence on others (human or non-human) to make decisions for them” (Thumlert et al., 2023, p. 26). The AI-generated Caesar tries to mimic primary sources to seem “authentic” and could easily be mistaken as true by a younger student. If a student believes the words of AI and submits a paper with evidence based on an AI-historical figure, they have created an accidental fabrication of history. My interaction with ChatGPT displays this risk, rather than parsing Caesar’s *Commentarii* or Cicero’s speeches, it creates its “own” passages to push its rhetoric onto users.

Production 3 taught me that prompt literacy would be a necessary skill in order to even think of using AI models like ChatGPT and avoid "mis/dis-information, harmful ideologies and anti-science sentiments that have become part and parcel of the current digital knowledge ecosystem" (Birhan, 2025, p. 55). By tightening my prompts, I began to steer the AI away from its *default assumptions* of clichés and dramatizations, toward a somewhat decent historical reconstruction (Thumlert, 2025, Lecture 3). It reveals how AI requires existing expertise to use, as without my historical background, I might not have fought with the AI so much about its inaccuracies. The screenshots throughout my text display this progression, from simple, dramatic, English-only responses filled with foreshadowing to a more scholarly-based, Latin speeches with translations. Thus, as "vendors market generative AI as a powerful technology with the capacity to solve a multitude of societal, structural, political and economic challenges", this expriement shows how AI algorithms dangerously push their own ideas, through simplicity and probability, onto its users, unless used with precise prompts (Birhan, 2025, p. 53). 

#### Conclusion
This activity showed me ChatGPT’s strengths and weaknesses. With strict prompting, it can generate creative reconstructions, language practice, or entertaining engagement with the past. On the other hand, without constant correction, its biases and reliance on cultural tropes, its algorithmic assumptions, and its tendency to hallucinate the truth are all risks for education and risk the normalization of disinformation (Thumlert, 2025, Lecture 3). Historical educators, if they must, should accept AI as a tool for critical inquiry/practice rather than as a source of historical truth. We should follow Thumlert et al.'s ideas and must understand not only what the AI produces but also why and how it produces it and Selwyn's quote that the "Use of AI systems and tools must be safe and effective for students" (Thumlert et al., 2023, p. 24–26 & Selwyn, 2024, p. 12). 

To conclude, I feel that an activity like this one, where one compares AI-generated content and real historical evidence, is a great way to warn/teach students and teachers alike of the risks of AI. Used critically, it can spark curiosity and reflection. Used uncritically, it risks simplifying history into cliché and de-skilling students and even teachers. Overall, in a world filled with mix discursive and rhetorical positions on GEN AI my experiment with ChatGPT reveals the falsehood behind bringing historical figures *back to life* for education. It exposes AI's algorithmic biases and default assumptions that distorts history, worse, it removes "the actuals actions...where learning happens" perpetuating colonial schooling systems by simplifying knowledge on Western sources and tropes, while silencing other voices and epistemologies that can address the problems of AI (Thumlert et al., 2023, p.30). 

## References

Birhan, A. (2025) The incomputable classroom: The limits and dangers of AI in education. In, *AI and the future of education.* Unesco. (Page 52). 

Selwyn, N. (2024). On the Limits of Artificial Intelligence (AI) in Education. *NTPK: Special Issue on Artificial Intelligence in Education*, 10, 3–14.

Thumlert, K., McBride, M., Tomin, B., Nolan, J., Lotherington, H., & Boreland, T. (2023). Algorithmic literacies: Identifying educational models and heuristics for engaging the challenge of algorithmic culture. *Digital Culture and Education.*

Thumlert, K. (2025, September 30). *Lecture 3 - Critical Algorithmic Literacies: Algorithmic Culture and GenAI. EDU 3610: New Media Literacies & Culture*. York University.

OpenAI. (2025). *ChatGPT* (Sept 22 version) [Large language model]. [https://chat.openai.com/](https://chat.openai.com/)
